{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defining project scope and objectives...\n",
      "Project timeline created: {'Project Planning': '02-06-2024 to 13-06-2024', 'Preprocessing Approaches': '14-06-2024 to 20-06-2024', 'Model Development': '21-06-2024 to 01-07-2024', 'Model Training and Evaluation': '02-07-2024 to 14-07-2024', 'Comparison and Analysis': '15-07-2024 to 22-07-2024', 'Documentation and Reporting': '23-07-2024 to 04-08-2024', 'Presentation and Review': '04-08-2024 to 15-08-2024'}\n",
      "Loading dataset...\n",
      "Dataset loaded. Preview:\n",
      "  LCLid\\tstdorToU\\tDateTime\\tKWH/hh (per half hour) \n",
      "0                      MAC005470\\tToU\\t00:00.0\\t0.18\n",
      "1                     MAC005470\\tToU\\t30:00.0\\t0.104\n",
      "2                     MAC005470\\tToU\\t00:00.0\\t0.118\n",
      "3                     MAC005470\\tToU\\t30:00.0\\t0.154\n",
      "4                     MAC005470\\tToU\\t00:00.0\\t0.026\n",
      "Column names in the dataset:\n",
      "Index(['LCLid\\tstdorToU\\tDateTime\\tKWH/hh (per half hour) '], dtype='object')\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'DateTime'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3621\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3620\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3621\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3622\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\_libs\\index.pyx:136\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\_libs\\index.pyx:163\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5198\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5206\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'DateTime'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[1;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 238>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    237\u001b[0m \u001b[38;5;66;03m# Main execution\u001b[39;00m\n\u001b[0;32m    238\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 239\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m \u001b[43mproject_planning\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    240\u001b[0m     preprocessing_approaches(dataset)\n\u001b[0;32m    241\u001b[0m     model_development()\n",
      "Input \u001b[1;32mIn [6]\u001b[0m, in \u001b[0;36mproject_planning\u001b[1;34m()\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumn names in the dataset:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     76\u001b[0m \u001b[38;5;28mprint\u001b[39m(dataset\u001b[38;5;241m.\u001b[39mcolumns)\n\u001b[1;32m---> 78\u001b[0m dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDateTime\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mDateTime\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[0;32m     79\u001b[0m dataset\u001b[38;5;241m.\u001b[39mset_index(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDateTime\u001b[39m\u001b[38;5;124m'\u001b[39m, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     80\u001b[0m dataset\u001b[38;5;241m.\u001b[39mindex \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(dataset\u001b[38;5;241m.\u001b[39mindex, errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcoerce\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\frame.py:3505\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3503\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   3504\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3505\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3506\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3507\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3623\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3621\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3622\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m-> 3623\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3624\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3625\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3626\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3627\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3628\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'DateTime'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Define the Random Forest Model\n",
    "class RandomForestModel:\n",
    "    def __init__(self):\n",
    "        self.model = RandomForestRegressor()\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self.model.fit(X, y)\n",
    "        \n",
    "    def predict(self, X):\n",
    "        return self.model.predict(X)\n",
    "\n",
    "# Define the SARIMA Model\n",
    "class SARIMAXModel:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.model = None\n",
    "        \n",
    "    def fit(self):\n",
    "        self.model = SARIMAX(self.data['KWH/hh (per half hour)'], order=(1, 1, 1), seasonal_order=(1, 1, 1, 24))\n",
    "        self.fit = self.model.fit(disp=False)\n",
    "        \n",
    "    def predict(self):\n",
    "        forecast = self.fit.forecast(steps=len(self.data))\n",
    "        conf_int = self.fit.get_forecast(steps=len(self.data)).conf_int()\n",
    "        return forecast, conf_int\n",
    "\n",
    "# Define the LSTM Model\n",
    "class LSTMModel:\n",
    "    def __init__(self, input_shape):\n",
    "        self.model = self.build_model(input_shape)\n",
    "        \n",
    "    def build_model(self, input_shape):\n",
    "        model = Sequential()\n",
    "        model.add(LSTM(50, return_sequences=True, input_shape=input_shape))\n",
    "        model.add(LSTM(50))\n",
    "        model.add(Dense(1))\n",
    "        model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "        return model\n",
    "        \n",
    "    def fit(self, X, y, epochs=50, batch_size=32):\n",
    "        self.model.fit(X, y, epochs=epochs, batch_size=batch_size)\n",
    "        \n",
    "    def predict(self, X):\n",
    "        return self.model.predict(X)\n",
    "\n",
    "# Function for project planning\n",
    "def project_planning():\n",
    "    print(\"Defining project scope and objectives...\")\n",
    "    timeline = {\n",
    "        \"Project Planning\": \"02-06-2024 to 13-06-2024\",\n",
    "        \"Preprocessing Approaches\": \"14-06-2024 to 20-06-2024\",\n",
    "        \"Model Development\": \"21-06-2024 to 01-07-2024\",\n",
    "        \"Model Training and Evaluation\": \"02-07-2024 to 14-07-2024\",\n",
    "        \"Comparison and Analysis\": \"15-07-2024 to 22-07-2024\",\n",
    "        \"Documentation and Reporting\": \"23-07-2024 to 04-08-2024\",\n",
    "        \"Presentation and Review\": \"04-08-2024 to 15-08-2024\"\n",
    "    }\n",
    "    print(\"Project timeline created:\", timeline)\n",
    "\n",
    "    print(\"Loading dataset...\")\n",
    "    dataset = pd.read_csv('LCL-June2015v2_167.csv')\n",
    "    print(\"Dataset loaded. Preview:\")\n",
    "    print(dataset.head())\n",
    "\n",
    "    print(\"Column names in the dataset:\")\n",
    "    print(dataset.columns)\n",
    "\n",
    "    dataset['DateTime'] = pd.to_datetime(dataset['DateTime'])\n",
    "    dataset.set_index('DateTime', inplace=True)\n",
    "    dataset.index = pd.to_datetime(dataset.index, errors='coerce')\n",
    "    dataset = dataset.resample('D').sum()\n",
    "    print(\"Data resampled to daily frequency.\")\n",
    "    print(\"Performing basic statistical analysis...\")\n",
    "    print(dataset.describe())\n",
    "    print(\"Milestone achieved: Project meeting scheduled.\")\n",
    "\n",
    "    return dataset\n",
    "\n",
    "# Function for preprocessing approaches\n",
    "def preprocessing_approaches(dataset):\n",
    "    print(\"Implementing advanced interpolation...\")\n",
    "    dataset.columns = dataset.columns.str.strip()  # Trim whitespace from column names\n",
    "    dataset['KWH/hh (per half hour)'] = pd.to_numeric(dataset['KWH/hh (per half hour)'], errors='coerce')\n",
    "    dataset['KWH/hh (per half hour)'] = dataset['KWH/hh (per half hour)'].replace(0, np.nan)\n",
    "    dataset['KWH/hh (per half hour)'] = dataset['KWH/hh (per half hour)'].interpolate(method='linear')\n",
    "    dataset['KWH/hh (per half hour)'] = dataset['KWH/hh (per half hour)'].fillna(0).astype(np.int64)\n",
    "\n",
    "    dataset.index = pd.to_datetime(dataset.index, errors='coerce')\n",
    "    print(\"Implementing feature engineering...\")\n",
    "    dataset['Hour'] = dataset.index.hour\n",
    "    dataset['Day'] = dataset.index.dayofweek\n",
    "    dataset.to_csv('preprocessed_dataset.csv', index=True)\n",
    "    print(\"Preprocessed dataset saved.\")\n",
    "    print(\"Milestone achieved: Preprocessed dataset ready for modeling.\")\n",
    "\n",
    "# Function for model development\n",
    "def model_development():\n",
    "    print(\"Developing models...\")\n",
    "    print(\"Milestone achieved: Initial model implementations completed.\")\n",
    "\n",
    "# Function for model training and evaluation\n",
    "def model_training_evaluation(dataset):\n",
    "    print(\"Training and evaluating models...\")\n",
    "\n",
    "    # Load the preprocessed dataset\n",
    "    dataset = pd.read_csv('preprocessed_dataset.csv', index_col='DateTime')\n",
    "\n",
    "    # Ensure datetime index is correctly parsed\n",
    "    dataset.index = pd.to_datetime(dataset.index)\n",
    "\n",
    "    # Slice the dataset for the last 30 days\n",
    "    end_date = dataset.index.max()\n",
    "    start_date = end_date - pd.Timedelta(days=100)\n",
    "    dataset = dataset.loc[start_date:end_date]\n",
    "\n",
    "    # Print column names to debug the issue\n",
    "    print(\"Columns in the dataset:\")\n",
    "    print(dataset.columns)\n",
    "\n",
    "    target_column = 'KWH/hh (per half hour)'  # Adjust based on actual column name\n",
    "\n",
    "    # Ensure 'Hour' and 'Day' columns exist\n",
    "    if 'Hour' not in dataset.columns or 'Day' not in dataset.columns:\n",
    "        raise KeyError(\"Required columns 'Hour' or 'Day' not found in the dataset.\")\n",
    "\n",
    "    X = dataset[['Hour', 'Day']]  # Features for model training\n",
    "\n",
    "    # Convert target variable to numeric\n",
    "    dataset[target_column] = pd.to_numeric(dataset[target_column], errors='coerce')\n",
    "\n",
    "    # Handle NaN values\n",
    "    y = dataset[target_column].dropna()\n",
    "    X = X.loc[y.index]\n",
    "\n",
    "    # Check for NaN values in the target variable\n",
    "    print(\"Checking for NaN values in target variable:\")\n",
    "    print(y.isna().sum())\n",
    "\n",
    "    # Print shapes of X and y\n",
    "    print(f\"Shape of X: {X.shape}, Shape of y: {y.shape}\")\n",
    "\n",
    "    if X.empty or y.empty:\n",
    "        print(\"Error: No valid data available for training. Please check the dataset.\")\n",
    "        return\n",
    "\n",
    "    # Train Random Forest model\n",
    "    rf_model = RandomForestModel()\n",
    "    rf_model.fit(X, y)\n",
    "    rf_predictions = rf_model.predict(X)\n",
    "\n",
    "    # Train SARIMA model\n",
    "    sarima_model = SARIMAXModel(dataset)\n",
    "    sarima_model.fit()\n",
    "    sarima_predictions, sarima_conf_int = sarima_model.predict()\n",
    "\n",
    "    # Evaluate models using MSE and MAE\n",
    "    rf_mse = mean_squared_error(y, rf_predictions)\n",
    "    rf_mae = mean_absolute_error(y, rf_predictions)\n",
    "    print(f\"Random Forest MSE: {rf_mse}, MAE: {rf_mae}\")\n",
    "\n",
    "    sarima_mse = mean_squared_error(y, sarima_predictions)\n",
    "    sarima_mae = mean_absolute_error(y, sarima_predictions)\n",
    "    print(f\"SARIMAX MSE: {sarima_mse}, MAE: {sarima_mae}\")\n",
    "\n",
    "    # Prepare data for LSTM\n",
    "    X_lstm = np.array(X).reshape((X.shape[0], 1, X.shape[1]))\n",
    "    lstm_model = LSTMModel(input_shape=(1, X.shape[1]))\n",
    "    lstm_model.fit(X_lstm, y.values)\n",
    "\n",
    "    lstm_predictions = lstm_model.predict(X_lstm)\n",
    "\n",
    "    lstm_mse = mean_squared_error(y, lstm_predictions)\n",
    "    lstm_mae = mean_absolute_error(y, lstm_predictions)\n",
    "    print(f\"LSTM MSE: {lstm_mse}, MAE: {lstm_mae}\")\n",
    "\n",
    "    # Plot Random Forest predictions\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    plt.plot(dataset.index, dataset['KWH/hh (per half hour)'], label='Actual Energy Usage', color='blue')\n",
    "    plt.plot(dataset.index, rf_predictions, label='RF Predictions', color='red')\n",
    "    plt.title('Random Forest Energy Usage Predictions (Last 30 Days)')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Energy Usage (kWh)')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Plot SARIMAX forecast with confidence intervals\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    plt.plot(dataset.index, dataset['KWH/hh (per half hour)'], label='Actual Energy Usage', color='blue')\n",
    "    plt.plot(dataset.index, sarima_predictions, label='SARIMAX Predictions', color='green')\n",
    "    plt.fill_between(dataset.index,\n",
    "                     sarima_conf_int.iloc[:, 0],\n",
    "                     sarima_conf_int.iloc[:, 1],\n",
    "                     color='green', alpha=0.3, label='Confidence Interval')\n",
    "    plt.title('SARIMAX Energy Usage Forecast with Confidence Intervals (Last 30 Days)')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Energy Usage (kWh)')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Plot LSTM forecast\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    plt.plot(dataset.index, dataset['KWH/hh (per half hour)'], label='Actual Energy Usage', color='blue')\n",
    "    plt.plot(dataset.index, lstm_predictions, label='LSTM Predictions', color='orange')\n",
    "    plt.title('LSTM Energy Usage Forecast (Last 30 Days)')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Energy Usage (kWh)')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    print(\"Milestone achieved: Model performance evaluation report completed.\")\n",
    "\n",
    "# Function for comparison analysis\n",
    "def comparison_analysis():\n",
    "    print(\"Comparing model performance across different preprocessing approaches...\")\n",
    "    print(\"Analysis completed.\")\n",
    "\n",
    "# Function for documentation and reporting\n",
    "def documentation_reporting():\n",
    "    print(\"Documenting methodologies and findings...\")\n",
    "    print(\"Draft project report and presentation ready.\")\n",
    "\n",
    "# Function for presentation and review\n",
    "def presentation_review():\n",
    "    print(\"Gathering feedback and refining report...\")\n",
    "    print(\"Final project report and presentation complete.\")\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    dataset = project_planning()\n",
    "    preprocessing_approaches(dataset)\n",
    "    model_development()\n",
    "    model_training_evaluation(dataset)\n",
    "    comparison_analysis()\n",
    "    documentation_reporting()\n",
    "    presentation_review()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
