{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'statsmodels'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mensemble\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RandomForestRegressor\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mean_squared_error, mean_absolute_error\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mstatsmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtsa\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstatespace\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msarimax\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SARIMAX\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Sequential\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LSTM, Dense\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'statsmodels'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Define the Random Forest Model\n",
    "class RandomForestModel:\n",
    "    def __init__(self):\n",
    "        self.model = RandomForestRegressor()\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self.model.fit(X, y)\n",
    "        \n",
    "    def predict(self, X):\n",
    "        return self.model.predict(X)\n",
    "\n",
    "# Define the SARIMA Model\n",
    "class SARIMAXModel:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.model = None\n",
    "        \n",
    "    def fit(self):\n",
    "        self.model = SARIMAX(self.data['KWH/hh (per half hour)'], order=(1, 1, 1), seasonal_order=(1, 1, 1, 24))\n",
    "        self.fit = self.model.fit(disp=False)\n",
    "        \n",
    "    def predict(self):\n",
    "        forecast = self.fit.forecast(steps=len(self.data))\n",
    "        conf_int = self.fit.get_forecast(steps=len(self.data)).conf_int()\n",
    "        return forecast, conf_int\n",
    "\n",
    "# Define the LSTM Model\n",
    "class LSTMModel:\n",
    "    def __init__(self, input_shape):\n",
    "        self.model = self.build_model(input_shape)\n",
    "        \n",
    "    def build_model(self, input_shape):\n",
    "        model = Sequential()\n",
    "        model.add(LSTM(50, return_sequences=True, input_shape=input_shape))\n",
    "        model.add(LSTM(50))\n",
    "        model.add(Dense(1))\n",
    "        model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "        return model\n",
    "        \n",
    "    def fit(self, X, y, epochs=50, batch_size=32):\n",
    "        self.model.fit(X, y, epochs=epochs, batch_size=batch_size)\n",
    "        \n",
    "    def predict(self, X):\n",
    "        return self.model.predict(X)\n",
    "\n",
    "# Function for project planning\n",
    "def project_planning():\n",
    "    print(\"Defining project scope and objectives...\")\n",
    "    timeline = {\n",
    "        \"Project Planning\": \"02-06-2024 to 13-06-2024\",\n",
    "        \"Preprocessing Approaches\": \"14-06-2024 to 20-06-2024\",\n",
    "        \"Model Development\": \"21-06-2024 to 01-07-2024\",\n",
    "        \"Model Training and Evaluation\": \"02-07-2024 to 14-07-2024\",\n",
    "        \"Comparison and Analysis\": \"15-07-2024 to 22-07-2024\",\n",
    "        \"Documentation and Reporting\": \"23-07-2024 to 04-08-2024\",\n",
    "        \"Presentation and Review\": \"04-08-2024 to 15-08-2024\"\n",
    "    }\n",
    "    print(\"Project timeline created:\", timeline)\n",
    "\n",
    "    print(\"Loading dataset...\")\n",
    "    dataset = pd.read_csv('LCL-June2015v2_167.csv')\n",
    "    print(\"Dataset loaded. Preview:\")\n",
    "    print(dataset.head())\n",
    "\n",
    "    print(\"Column names in the dataset:\")\n",
    "    print(dataset.columns)\n",
    "\n",
    "    dataset['DateTime'] = pd.to_datetime(dataset['DateTime'])\n",
    "    dataset.set_index('DateTime', inplace=True)\n",
    "    dataset.index = pd.to_datetime(dataset.index, errors='coerce')\n",
    "    dataset = dataset.resample('D').sum()\n",
    "    print(\"Data resampled to daily frequency.\")\n",
    "    print(\"Performing basic statistical analysis...\")\n",
    "    print(dataset.describe())\n",
    "    print(\"Milestone achieved: Project meeting scheduled.\")\n",
    "\n",
    "    return dataset\n",
    "\n",
    "# Function for preprocessing approaches\n",
    "def preprocessing_approaches(dataset):\n",
    "    print(\"Implementing advanced interpolation...\")\n",
    "    dataset.columns = dataset.columns.str.strip()  # Trim whitespace from column names\n",
    "    dataset['KWH/hh (per half hour)'] = pd.to_numeric(dataset['KWH/hh (per half hour)'], errors='coerce')\n",
    "    dataset['KWH/hh (per half hour)'] = dataset['KWH/hh (per half hour)'].replace(0, np.nan)\n",
    "    dataset['KWH/hh (per half hour)'] = dataset['KWH/hh (per half hour)'].interpolate(method='linear')\n",
    "    dataset['KWH/hh (per half hour)'] = dataset['KWH/hh (per half hour)'].fillna(0).astype(np.int64)\n",
    "\n",
    "    dataset.index = pd.to_datetime(dataset.index, errors='coerce')\n",
    "    print(\"Implementing feature engineering...\")\n",
    "    dataset['Hour'] = dataset.index.hour\n",
    "    dataset['Day'] = dataset.index.dayofweek\n",
    "    dataset.to_csv('preprocessed_dataset.csv', index=True)\n",
    "    print(\"Preprocessed dataset saved.\")\n",
    "    print(\"Milestone achieved: Preprocessed dataset ready for modeling.\")\n",
    "\n",
    "# Function for model development\n",
    "def model_development():\n",
    "    print(\"Developing models...\")\n",
    "    print(\"Milestone achieved: Initial model implementations completed.\")\n",
    "\n",
    "# Function for model training and evaluation\n",
    "def model_training_evaluation(dataset):\n",
    "    print(\"Training and evaluating models...\")\n",
    "\n",
    "    # Load the preprocessed dataset\n",
    "    dataset = pd.read_csv('preprocessed_dataset.csv', index_col='DateTime')\n",
    "\n",
    "    # Ensure datetime index is correctly parsed\n",
    "    dataset.index = pd.to_datetime(dataset.index)\n",
    "\n",
    "    # Slice the dataset for the last 30 days\n",
    "    end_date = dataset.index.max()\n",
    "    start_date = end_date - pd.Timedelta(days=100)\n",
    "    dataset = dataset.loc[start_date:end_date]\n",
    "\n",
    "    # Print column names to debug the issue\n",
    "    print(\"Columns in the dataset:\")\n",
    "    print(dataset.columns)\n",
    "\n",
    "    target_column = 'KWH/hh (per half hour)'  # Adjust based on actual column name\n",
    "\n",
    "    # Ensure 'Hour' and 'Day' columns exist\n",
    "    if 'Hour' not in dataset.columns or 'Day' not in dataset.columns:\n",
    "        raise KeyError(\"Required columns 'Hour' or 'Day' not found in the dataset.\")\n",
    "\n",
    "    X = dataset[['Hour', 'Day']]  # Features for model training\n",
    "\n",
    "    # Convert target variable to numeric\n",
    "    dataset[target_column] = pd.to_numeric(dataset[target_column], errors='coerce')\n",
    "\n",
    "    # Handle NaN values\n",
    "    y = dataset[target_column].dropna()\n",
    "    X = X.loc[y.index]\n",
    "\n",
    "    # Check for NaN values in the target variable\n",
    "    print(\"Checking for NaN values in target variable:\")\n",
    "    print(y.isna().sum())\n",
    "\n",
    "    # Print shapes of X and y\n",
    "    print(f\"Shape of X: {X.shape}, Shape of y: {y.shape}\")\n",
    "\n",
    "    if X.empty or y.empty:\n",
    "        print(\"Error: No valid data available for training. Please check the dataset.\")\n",
    "        return\n",
    "\n",
    "    # Train Random Forest model\n",
    "    rf_model = RandomForestModel()\n",
    "    rf_model.fit(X, y)\n",
    "    rf_predictions = rf_model.predict(X)\n",
    "\n",
    "    # Train SARIMA model\n",
    "    sarima_model = SARIMAXModel(dataset)\n",
    "    sarima_model.fit()\n",
    "    sarima_predictions, sarima_conf_int = sarima_model.predict()\n",
    "\n",
    "    # Evaluate models using MSE and MAE\n",
    "    rf_mse = mean_squared_error(y, rf_predictions)\n",
    "    rf_mae = mean_absolute_error(y, rf_predictions)\n",
    "    print(f\"Random Forest MSE: {rf_mse}, MAE: {rf_mae}\")\n",
    "\n",
    "    sarima_mse = mean_squared_error(y, sarima_predictions)\n",
    "    sarima_mae = mean_absolute_error(y, sarima_predictions)\n",
    "    print(f\"SARIMAX MSE: {sarima_mse}, MAE: {sarima_mae}\")\n",
    "\n",
    "    # Prepare data for LSTM\n",
    "    X_lstm = np.array(X).reshape((X.shape[0], 1, X.shape[1]))\n",
    "    lstm_model = LSTMModel(input_shape=(1, X.shape[1]))\n",
    "    lstm_model.fit(X_lstm, y.values)\n",
    "\n",
    "    lstm_predictions = lstm_model.predict(X_lstm)\n",
    "\n",
    "    lstm_mse = mean_squared_error(y, lstm_predictions)\n",
    "    lstm_mae = mean_absolute_error(y, lstm_predictions)\n",
    "    print(f\"LSTM MSE: {lstm_mse}, MAE: {lstm_mae}\")\n",
    "\n",
    "    # Plot Random Forest predictions\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    plt.plot(dataset.index, dataset['KWH/hh (per half hour)'], label='Actual Energy Usage', color='blue')\n",
    "    plt.plot(dataset.index, rf_predictions, label='RF Predictions', color='red')\n",
    "    plt.title('Random Forest Energy Usage Predictions (Last 30 Days)')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Energy Usage (kWh)')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Plot SARIMAX forecast with confidence intervals\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    plt.plot(dataset.index, dataset['KWH/hh (per half hour)'], label='Actual Energy Usage', color='blue')\n",
    "    plt.plot(dataset.index, sarima_predictions, label='SARIMAX Predictions', color='green')\n",
    "    plt.fill_between(dataset.index,\n",
    "                     sarima_conf_int.iloc[:, 0],\n",
    "                     sarima_conf_int.iloc[:, 1],\n",
    "                     color='green', alpha=0.3, label='Confidence Interval')\n",
    "    plt.title('SARIMAX Energy Usage Forecast with Confidence Intervals (Last 30 Days)')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Energy Usage (kWh)')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Plot LSTM forecast\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    plt.plot(dataset.index, dataset['KWH/hh (per half hour)'], label='Actual Energy Usage', color='blue')\n",
    "    plt.plot(dataset.index, lstm_predictions, label='LSTM Predictions', color='orange')\n",
    "    plt.title('LSTM Energy Usage Forecast (Last 30 Days)')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Energy Usage (kWh)')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    print(\"Milestone achieved: Model performance evaluation report completed.\")\n",
    "\n",
    "# Function for comparison analysis\n",
    "def comparison_analysis():\n",
    "    print(\"Comparing model performance across different preprocessing approaches...\")\n",
    "    print(\"Analysis completed.\")\n",
    "\n",
    "# Function for documentation and reporting\n",
    "def documentation_reporting():\n",
    "    print(\"Documenting methodologies and findings...\")\n",
    "    print(\"Draft project report and presentation ready.\")\n",
    "\n",
    "# Function for presentation and review\n",
    "def presentation_review():\n",
    "    print(\"Gathering feedback and refining report...\")\n",
    "    print(\"Final project report and presentation complete.\")\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    dataset = project_planning()\n",
    "    preprocessing_approaches(dataset)\n",
    "    model_development()\n",
    "    model_training_evaluation(dataset)\n",
    "    comparison_analysis()\n",
    "    documentation_reporting()\n",
    "    presentation_review()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
